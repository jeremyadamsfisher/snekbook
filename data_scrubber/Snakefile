rule recommender_system:
    input: "snake_db.csv"
    output: "snake_db_enriched.csv"
    run:
        import pandas as pd
        from sklearn.feature_extraction.text import CountVectorizer
        from sklearn.metrics.pairwise import cosine_similarity

        df = pd.read_csv(input[0])
        soup = df.apply((lambda row: " ".join(str(elem) for elem in row if elem)), axis=1)
        count = CountVectorizer(stop_words='english')
        count_matrix = count.fit_transform(soup)
        pairwise_cosine_simularity = cosine_similarity(count_matrix, count_matrix)

        def get_snake_recommendations(snake_id, n=10):
            best_snake_ids, _ = zip(*sorted(
                enumerate(pairwise_cosine_simularity[snake_id]),
                key=(lambda id_cos_val: id_cos_val[1]),
                reverse=True
            ))
            return best_snake_ids[1:n+1]

        df["recommended_snakes"] = df.reset_index()["index"].apply(get_snake_recommendations)
        df.to_csv(output[0])

rule download_snake_html:
    output: temporary("snake_db.html")
    run:
        from selenium import webdriver
        from bs4 import BeautifulSoup
        import time

        driver = webdriver.Safari()
        driver.get("http://snakedatabase.org/pages/filterdb.php")
        driver.find_element_by_tag_name("form").submit()
        time.sleep(5)

        with open(output[0], "wt") as f:
            f.write(driver.page_source)

rule convert_snake_html_to_csv:
    input: "snake_db.html"
    output: temporary("snake_db_raw.csv")
    run:
        from bs4 import BeautifulSoup
        import csv
        with open(input[0]) as f, \
            open(output[0], "wt") as f_out:

            w = csv.writer(f_out)
            w.writerow(['scientific_name', 'fangs', 'toxicity', 'rating', 'common name', 'tbl_max', '_'])

            soup = BeautifulSoup(f.read(), "lxml")
            main_table = soup.find("table", {"class": "sortable"})
            trs = main_table.find_all("tr")

            for tr in trs:
                rec = [td.text.strip("\n").strip() for td in tr.find_all("td") if td.text.strip("\n")]
                if rec:
                    w.writerow(rec)

rule clean_snake_csv:
    input: "snake_db_raw.csv"
    output: "snake_db.csv"
    run:
        import csv
        rating = {
            '???': '???',
            '---': 'unclear dangerousness',
            '?ğŸ?': 'constrictor unclear dangerousness',
            'ğŸ': 'constrictor considered harmless',
            'ğŸğŸ': 'constrictor harmful',
            'ğŸğŸğŸ': 'constrictor dangerous',
            'ğŸğŸğŸğŸ': 'constrictor very dangerous',
            'ğŸğŸğŸğŸğŸ':'constrictor extremely dangerous',
            '?ğŸ’€?': 'unclear dangerousness',
            'ğŸ’€': 'considered harmless',
            'ğŸ’€ğŸ’€': 'harmful bite',
            'ğŸ’€ğŸ’€ğŸ’€': 'dangerous bite',
            'ğŸ’€ğŸ’€ğŸ’€ğŸ’€': 'very dangerous bite',
            'ğŸ’€ğŸ’€ğŸ’€ğŸ’€ğŸ’€': 'extremely dangerous bite'
        }
        with open(input[0]) as f, open(output[0], "wt") as f_out:
            db_reader = csv.DictReader(f)
            db_writer = csv.DictWriter(f_out, fieldnames=[
                'genus',
                'species',
                'fangs',
                'toxicity',
                'rating',
                'common name'
            ])
            db_writer.writeheader()
            for row in db_reader:
                try:
                    genus, species = row["scientific_name"].replace("\xa0", " ").split(" ")
                    db_writer.writerow({
                        "genus": genus,
                        "species": species,
                        "fangs": row["fangs"].replace("ref", ""),
                        "toxicity": row["toxicity"].replace("ref", ""),
                        "rating": rating[row["rating"].replace("ref", "")],
                        "common name": row["common name"]
                    })
                except ValueError:
                    raise ValueError("Corrupted row: {}".format(row["scientific_name"].replace("\xa0", " ").split(" ")))
